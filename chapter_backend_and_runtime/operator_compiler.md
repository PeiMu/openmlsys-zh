## 算子编译器（扩展部分）

算子编译器，顾名思义，即对“单个算子”进行编译优化的工具。这里所谓的“单个算子”可以来自于整个神经网络中的一部分，也可以来自于通过[领域特定语言（Domain Specific Language, DSL）](https://en.wikipedia.org/wiki/Domain-specific_language)实现的代码。而所谓编译，通俗来说起到的是针对目标语言进行**表达**和**转换**。

从目的上来说，算子编译器致力于提高单个算子的**执行性能**并同时降低**编译耗时**。从工程实现上来说，算子编译器的输入一般为python等**动态语言**描述的张量计算，而输出一般为**特定AI芯片**上的可执行文件。

针对上述两个方面进行深入思考，我们就会从中发现当前算子编译器面对的挑战：

1. 为了提升性能，我们该如何选择适合的数据与指令调度方式？
2. 当算子编译器面对多种体系结构特点的后端芯片时，我们该如何保证性能调优策略的泛化能力和适配性问题？
3. 当算子编译器面对多种不同特点的前端框架或输入语言时，我们该如何保证完整而准确地表达它们？

带着上述问题，我们下面逐一进行讨论与探索，并结合当下流行的算子编译器进行举例和引证。

### 采用怎样的调度策略？

编译器如果不考虑优化和实际中芯片的体系结构特点，我们只需要把输入进来的张量计算表达式全部加载进计算核心里完成计算，之后再把计算结果从计算核心里面取出并保存下来即可。但事实上，我们有：“处理器在短时间内重复访问同一内存位置时效率高”这一[局部性概念](https://en.wikipedia.org/wiki/Locality_of_reference)；以及“计算任务总量一定时，同时并行计算的任务量越多，总耗时最少”这一[并行性概念](https://en.wikipedia.org/wiki/Parallel_computing)。基于以上两个基本概念，我们为了尽可能提高程序执行性能，提出了通过调整每次计算数据的大小，改变部分数据计算的先后顺序，以及针对不同数据流的特点放进不同计算核等方法。以上种种在程序实际运行的时候针对数据做出的特殊操作，我们统称为调度（schedule）。

业界的先行者[TVM](https://github.com/apache/tvm)将调度的概念再度抽象，提出了[调度原语（schedule primitives）](https://tvm.apache.org/docs/how_to/work_with_schedules/schedule_primitives.html?highlight=schedule%20primitives)的概念。而在具体实践中TVM将这些像积木一样的原语进行自动组合，提出了自动调度的算法Ansor :cite:`zheng2020ansor`。华为TBE提出依据专家知识，将这些原语手动组合形成模板，从而达到性能更优的效果。然而，业界另一编译器框架[MLIR](https://mlir.llvm.org/)做法则不同。它并没有明确提出调度抽象的概念，而是将各种调度优化的方法分散进进编译器中段里，并针对多重循环中丰富而复杂的调度优化使用了多面体模型编译（polyhedron compilation）技术 :cite:`grosser2011polly`。

以上这些方法在设计哲学上各有不同；在实际应用中的不同场景下也各有优劣。而它们在持续的发展中也在互相借鉴，希望几年后可以看到一个收敛的解决方案。

### 如何选择出最优的切分参数？

通过前面提到的利用局部性和并行性进行优化的思路，我们可以利用这两个性质将输入的张量切的小一点。这样一来，切小后的数据块能够放入缓存（cache）以充分利用局部性，或者切出的几个数据块之间没有依赖关系从而能够进行并行计算。由此看来，如何切分（tile）尤为重要。

关于切分方法，我们可以采用上述两个性质针对目标AI芯片的内存结构特点进行建模，从数学上推导出最优的一组解，这一方法我们称为自动切分（auto tiling）算法。另外，我们也可以通过设计一套合理的仿真模型（simulation model）和开销模型（cost model）把所有可能的参数挨个运行一遍，这一方法我们称为自动调优（auto tuning）方法。总之，或计算或运行，能够在可接受的时间内得到一个最优或相对较优的切分参数是我们最大的目标！

### 如何适配不同的AI芯片？

一般意义上来说，通用编译器的设计会尽量适配多种后端。如此一来，在面临不同体系结构特点和不同编程模型的多种后端时，算子编译器承受了相当大的压力。

当下的AI芯片中，常见的编程模型分为[单指令多数据（Single instruction, multiple data, SIMD）](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)和[单指令多线程（Single instruction, multiple threads, SIMT）](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)。前者对应的是带有向量计算指令的芯片，比如华为的昇腾系列芯片等；后者对应的是带有明显的线程分级的芯片，比如英伟达的V系列和A系列芯片等。另外，也有一些芯片开始结合这两种编程模型的特点，像寒武纪的思元系列芯片，既有类似线程并行计算的概念，又有向量指令的支持。针对不同的编程模型，算子编译器在进行优化（如向量化等）时的策略也会有所不同。

那么面对多芯片的复杂情况要如何权衡不同优化算法的实现呢？MLIR允许用户自由地选择优化组合，针对不同的目标芯片选择其心目中最优的优化方法，之后借助[LLVM](https://github.com/llvm/llvm-project)后端非常方便地生成如[CUDA](https://en.wikipedia.org/wiki/CUDA)，[NEON](https://en.wikipedia.org/wiki/ARM_architecture_family#Advanced_SIMD_(Neon))，C等代码。TVM采用一套固定的优化流程方案，但是可以在前端算子代码里自由配置是否开启相关的优化项。从某种意义上来说：MLIR的做法更自由，可以按照自己的理解组合出最适合自己目标芯片的优化组合，但是使用起来难度更高；TVM的流程更固定而简单，只需要修改配置项即可，但是相对没有那么灵活。

### 如何表达的准确而完整？

假设我们已经解决了上述调度优化和多芯片适配的问题，看起来似乎一切已经非常完美。然而我们还需要考虑到在实际场景中，AI编译器的输入常以[pytorch](https://github.com/pytorch/pytorch)代码居多，即此输入带有大量python的灵活表达方式（包括而不限于索引、view语义等）。另外在检测网络中，输入算子往往还有大量的控制流语句。此外，还经常可以看到神经网络中存在许多的动态形状问题，即网络中的算子形状会受网络迭代次数和控制流等条件的影响。这些都对算子编译器前端的表达能力提出了很高的要求。

在实际工程实践中，我们发现大量的长尾分布般不常见但性能很差的算子（后文简称为长尾算子）往往是整体网络训练或推理的瓶颈点。而这些长尾算子大都是由于其出现频次低而不至于实现在计算库中。同时其语法过于灵活或存在大量的控制流语句以及动态形状问题而难以被目前的算子编译器前端充分表达出来，因此也难以通过算子编译器进行优化加速。于是，这些长尾算子只好以运行速度较慢的python解释器或者虚拟机的方式执行，从而成为整个网络中的性能瓶颈。此时，提高算子编译器前端的表达能力就成为了重中之重。

[torch-mlir](https://github.com/llvm/torch-mlir)是一个前端承袭自pytorch，基于MLIR开发出的AI编译器。由于其前端承袭自pytorch，因此它可以直接利用pytorch带来的充分转换python语义及灵活表达性的优点。torch-mlir只需要将pytorch的中间表达（如torch-script）转换为自身需要的MLIR形式即可。TVM则通过其[图层编译器Relay的表达能力](https://tvm.apache.org/docs/reference/langref/relay_expr.html)将控制流转换到虚拟机进行执行，并在算子编译器部分通过[混合脚本(hybrid script)](https://tvm.apache.org/docs/reference/langref/hybrid_script.html)进行辅助表达。以上两个增强前端表达能力的优秀思路非常值得我们学习与思考！